{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7KwHWuyowcua","executionInfo":{"status":"ok","timestamp":1710199211702,"user_tz":240,"elapsed":18473,"user":{"displayName":"Luis Stanley Jovel Portal","userId":"08916066368548829529"}},"outputId":"0d4a363b-f7b5-497a-d237-31cb06a73f65"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Documents/Fulbright Application 2020-2021/Courses/Spring Semester 2024/Deep Decision and Reinforcement Learning/project\n","circle_clock.json         never_seen.json  recover_3.json  recover_6.json\n","circle_counterclock.json  recover_1.json   recover_4.json  snake_2.json\n","eight.json                recover_2.json   recover_5.json  snake.json\n"]}],"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd /content/drive/MyDrive/Documents/Fulbright\\ Application\\ 2020-2021/Courses/'Spring Semester 2024'/'Deep Decision and Reinforcement Learning'/project\n","%ls demos"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":581356,"status":"ok","timestamp":1710199793661,"user":{"displayName":"Luis Stanley Jovel Portal","userId":"08916066368548829529"},"user_tz":240},"id":"f7uCB7Crx66y","outputId":"f8128fb4-bbe8-491a-e3ba-ea43df0ad941"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]},{"output_type":"stream","name":"stderr","text":["Training:   5%|▌         | 1/20 [00:26<08:16, 26.13s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  1 loss:  0.031370244686480475\n"]},{"output_type":"stream","name":"stderr","text":["Training:  10%|█         | 2/20 [00:53<08:03, 26.87s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  2 loss:  0.02829996175330645\n"]},{"output_type":"stream","name":"stderr","text":["Training:  15%|█▌        | 3/20 [01:22<07:50, 27.68s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  3 loss:  0.0236359830148733\n"]},{"output_type":"stream","name":"stderr","text":["Training:  20%|██        | 4/20 [01:50<07:26, 27.88s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  4 loss:  0.02417366156662788\n"]},{"output_type":"stream","name":"stderr","text":["Training:  25%|██▌       | 5/20 [02:18<07:00, 28.01s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  5 loss:  0.022788035235818294\n"]},{"output_type":"stream","name":"stderr","text":["Training:  30%|███       | 6/20 [02:46<06:33, 28.12s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  6 loss:  0.021704312111698806\n"]},{"output_type":"stream","name":"stderr","text":["Training:  35%|███▌      | 7/20 [03:16<06:12, 28.67s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  7 loss:  0.02051477834593169\n"]},{"output_type":"stream","name":"stderr","text":["Training:  40%|████      | 8/20 [03:45<05:43, 28.64s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  8 loss:  0.020970944850375643\n"]},{"output_type":"stream","name":"stderr","text":["Training:  45%|████▌     | 9/20 [04:13<05:15, 28.66s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  9 loss:  0.021274267733078223\n"]},{"output_type":"stream","name":"stderr","text":["Training:  50%|█████     | 10/20 [04:42<04:47, 28.72s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  10 loss:  0.0219746627804219\n"]},{"output_type":"stream","name":"stderr","text":["Training:  55%|█████▌    | 11/20 [05:12<04:20, 28.96s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  11 loss:  0.014420931835327512\n"]},{"output_type":"stream","name":"stderr","text":["Training:  60%|██████    | 12/20 [05:42<03:55, 29.41s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  12 loss:  0.013837602496232464\n"]},{"output_type":"stream","name":"stderr","text":["Training:  65%|██████▌   | 13/20 [06:11<03:24, 29.19s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  13 loss:  0.01318635954109194\n"]},{"output_type":"stream","name":"stderr","text":["Training:  70%|███████   | 14/20 [06:40<02:54, 29.02s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  14 loss:  0.012540544877934028\n"]},{"output_type":"stream","name":"stderr","text":["Training:  75%|███████▌  | 15/20 [07:08<02:24, 28.87s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  15 loss:  0.01227319124808699\n"]},{"output_type":"stream","name":"stderr","text":["Training:  80%|████████  | 16/20 [07:37<01:55, 28.79s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  16 loss:  0.012005609801290782\n"]},{"output_type":"stream","name":"stderr","text":["Training:  85%|████████▌ | 17/20 [08:07<01:27, 29.13s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  17 loss:  0.011912529677685694\n"]},{"output_type":"stream","name":"stderr","text":["Training:  90%|█████████ | 18/20 [08:35<00:57, 28.97s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  18 loss:  0.01157502455961832\n"]},{"output_type":"stream","name":"stderr","text":["Training:  95%|█████████▌| 19/20 [09:04<00:28, 28.84s/it]  "]},{"output_type":"stream","name":"stdout","text":["epoch:  19 loss:  0.011254866635530568\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 20/20 [09:32<00:00, 28.64s/it]"]},{"output_type":"stream","name":"stdout","text":["epoch:  20 loss:  0.010946448775497783\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import os\n","import json\n","from enum import Enum\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, random_split\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from tqdm import tqdm\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","class BehaviorCloningModel(nn.Module):\n","    def __init__(self, num_history, num_features, output_size):\n","        super(BehaviorCloningModel, self).__init__()\n","        self.flattened_size = 64 * (num_features // 4)\n","        self.policy = nn.Sequential(\n","            nn.Conv1d(in_channels=num_history, out_channels=32, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.BatchNorm1d(32),\n","\n","            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.BatchNorm1d(64)\n","        )\n","        self.classifier = nn.Sequential(\n","            # Calculate the size after convolution and pooling\n","            nn.Linear(self.flattened_size, self.flattened_size),\n","            nn.ReLU(),\n","            nn.Linear(self.flattened_size, self.flattened_size),\n","            nn.ReLU(),\n","            nn.Linear(self.flattened_size, 128),\n","            nn.Dropout(),\n","            nn.ReLU(),\n","            nn.Linear(128, output_size),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.policy(x)\n","        x = x.view(-1, self.flattened_size)  # Flatten the tensor for the fully connected layer\n","        x = self.classifier(x)\n","        return x\n","\n","class Constants(Enum):\n","    INPUT_SIZE = 25  # Number of features in observation\n","    HIDDEN_SIZE = 128  # Number of units in hidden layer\n","    NUM_HISTORY = 12  # Number of history steps to use\n","    OUTPUT_SIZE = 2  # Number of actions\n","    DROPOUT = 0.25  # Dropout rate\n","    lr = 1e-3  # Learning rate\n","    EPOCHS = 20  # Number of epochs to train\n","\n","    NUM_LAYERS = 4  # Number of LSTM layers\n","\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, obs_list, action_list, num_history):\n","        self.obs_list = obs_list\n","        self.action_list = action_list\n","        self.num_history = num_history\n","\n","    def __len__(self):\n","        return len(self.obs_list)\n","\n","    def __getitem__(self, idx):\n","        obs = self.obs_list[idx]\n","        action = self.action_list[idx]\n","\n","        # Retrieve history observations\n","        start_idx = max(0, idx - self.num_history)\n","        history_obs = self.obs_list[start_idx:idx]\n","\n","        # Pad history observations if necessary\n","        if len(history_obs) < self.num_history:\n","            pad_width = self.num_history - len(history_obs)\n","            padding = torch.zeros(pad_width, Constants.INPUT_SIZE.value)\n","            history_obs = torch.cat([padding, history_obs])\n","\n","        return history_obs, action\n","\n","obs_list = torch.tensor([])\n","action_list = torch.tensor([])\n","\n","for file in os.listdir(\"./demos\"):\n","    if file.startswith(\"*\") or file.startswith(\".\"):\n","        continue\n","    with open(f\"./demos/{file}\", \"r\") as f:\n","        data = json.load(f)\n","        for episode in data:\n","            min_length = min(len(episode[0]), len(episode[1]))\n","            obs = episode[0][:min_length]\n","            action = episode[1][:min_length]\n","\n","            if len(obs) == 0 or len(action) == 0:\n","                continue\n","\n","            obs = torch.tensor(obs, dtype=torch.float32)\n","            action = torch.tensor(action, dtype=torch.float32)\n","            obs_list = torch.cat([obs_list, obs])\n","            action_list = torch.cat([action_list, action])\n","\n","dataset = Dataset(obs_list, action_list, Constants.NUM_HISTORY.value)\n","train_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_size\n","train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Instantiate model, loss function, and optimizer\n","# model = MultiHistoryNetwork(\n","#     Constants.INPUT_SIZE.value,\n","#     Constants.HIDDEN_SIZE.value,\n","#     Constants.OUTPUT_SIZE.value,\n","#     Constants.NUM_HISTORY.value).to(device)\n","\n","model = BehaviorCloningModel(\n","    Constants.NUM_HISTORY.value,\n","    Constants.INPUT_SIZE.value,\n","    Constants.OUTPUT_SIZE.value\n",").to(device)\n","\n","# create a loss function\n","loss_fn = nn.MSELoss().to(device)\n","\n","# create an optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=Constants.lr.value)\n","scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=2)\n","# train the model\n","iterator = tqdm(range(1, Constants.EPOCHS.value + 1), total=Constants.EPOCHS.value, desc=\"Training\")\n","\n","for epoch in iterator:\n","    model.train()\n","    iterator.set_description(\"Training\")\n","    for obs, action in train_dataloader:\n","        optimizer.zero_grad()\n","        obs = obs.to(device)\n","        action = action.to(device)\n","        pred = model(obs)\n","        loss = loss_fn(pred, action)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # evaluate the model\n","    iterator.set_description(\"Evaluating\")\n","    model.eval()\n","    with torch.no_grad():\n","        test_loss = 0\n","        for obs, action in test_dataloader:\n","            obs = obs.to(device)\n","            action = action.to(device)\n","            pred = model(obs)\n","            test_loss += loss_fn(pred, action).item()\n","        test_loss /= len(test_dataloader)\n","    # iterator.set_postfix(epoch=epoch, loss=test_loss)\n","    print('epoch: ', epoch, 'loss: ', test_loss)\n","    scheduler.step(test_loss)\n","\n","# save the model\n","torch.save(model.state_dict(), \"model_dict.pt\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOd46SZcPDTB0Jcho1poQcE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}